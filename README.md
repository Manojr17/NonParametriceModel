Non-Parametric Models are:

       -A non-parametric model is a type of model that does not assume a fixed functional form for the underlying data distribution. 
       -Instead of learning a finite set of parameters (like in parametric models), non-parametric models can adapt their complexity based on the data

Examples of Non-Parametric Models:

    i)k-Nearest Neighbors (k-NN):
          â€“ Stores training data and makes predictions based on nearest neighbors.
    ii)Decision Trees :
          â€“ Can grow dynamically based on data complexity.
    iii)Random Forests:
          â€“ An ensemble of decision trees.
    iv)Support Vector Machines (with Kernel Trick):
          â€“ Uses the data to create flexible decision boundaries.
    v)Kernel Density Estimation (KDE) :
          â€“ Estimates probability distributions without assuming a specific form.

1)Support Vector Regression (SVR) :

         -Support Vector Regression (SVR) is a type of regression model based on Support Vector Machines (SVMs).
         -Unlike standard regression models that aim to minimize the error directly, SVR tries to fit the best hyperplane within a certain margin of tolerance (Îµ). 
         -It is particularly useful when dealing with small datasets, high-dimensional data, and non-linearity.
When to Use SVR?

        âœ… When data is non-linear (use kernels like RBF).
        âœ… When the dataset is small (SVR works well with limited data).
        âœ… When you want robustness to outliers (controlled by C and Îµ).
        âœ… When prediction accuracy is more important than interpretability.

2) Decision Tree(DTR):
   
       -A Decision Tree is a supervised learning algorithm used for classification and regression tasks.
       - It works by recursively splitting the dataset into subsets based on feature values, forming a tree-like structure.

       -Each node in the tree represents a decision rule, and each branch corresponds to an outcome of that rule. 
        -The process continues until the data is classified, or a stopping condition is met.

  Key Terminologies in Decision Trees:
  
        *Root Node: The starting point of the tree, representing the entire dataset.
        *Internal Nodes: Nodes that split into branches based on decision criteria.
        *Leaf Nodes: Terminal nodes representing the final output (classification or regression value).
        *Depth of Tree: The longest path from the root node to a leaf node.
        *Pruning: The process of reducing tree size to avoid overfitting.

 When to Use Decision Trees?
 
        âœ… When interpretability is important (e.g., medical diagnoses).
        âœ… When the dataset contains both categorical and numerical features.
        âœ… For feature selection â€“ Decision trees can identify the most important features.
        âœ… For quick training and inference on small-to-medium datasets.

3)Random Forest Regression(RFT):

      - Random Forest is a supervised learning algorithm that combines multiple decision trees to create a more accurate and robust predictive model.
      -It is commonly used for both classification and regression tasks.

      -Instead of relying on a single decision tree, Random Forest builds multiple trees and combines their results to reduce overfitting and improve accuracy.

Key Hyperparameters in Random Forest:

      ðŸ”¹ n_estimators â€“ Number of trees in the forest.
      ðŸ”¹ max_depth â€“ Maximum depth of individual trees (prevents overfitting).
      ðŸ”¹ min_samples_split â€“ Minimum samples required to split a node.
      ðŸ”¹ min_samples_leaf â€“ Minimum samples required at a leaf node.
      ðŸ”¹ max_features â€“ Number of features considered at each split (e.g., "sqrt" for classification).
      ðŸ”¹ bootstrap â€“ Whether to use bootstrap sampling (True by default).

When to Use Random Forest?

      âœ… When you need high accuracy and are okay with slower training.
      âœ… When dealing with imbalanced datasets (tuning class weights helps).
      âœ… When you have missing data or a mix of categorical and numerical features.
      âœ… When interpretability is not a major concern.



